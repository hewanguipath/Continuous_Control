{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.7 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Crawler/Crawler_Linux_NoVis.zip\n",
    "#!unzip -qo Crawler_Linux_NoVis.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "env = UnityEnvironment(file_name='Crawler_Linux_NoVis/Crawler.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n",
      "Size of each action: 20\n",
      "There are 12 agents. Each observes a state with length: 129\n",
      "The state for the first agent looks like: [  0.00000000e+00   0.00000000e+00   0.00000000e+00   2.25000000e+00\n",
      "   1.00000000e+00   0.00000000e+00   1.78813934e-07   0.00000000e+00\n",
      "   1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.06093168e-01  -1.42857209e-01  -6.06078804e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.33339906e+00  -1.42857209e-01\n",
      "  -1.33341408e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -6.06093347e-01  -1.42857209e-01  -6.06078625e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.33339953e+00  -1.42857209e-01\n",
      "  -1.33341372e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -6.06093168e-01  -1.42857209e-01   6.06078804e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.33339906e+00  -1.42857209e-01\n",
      "   1.33341408e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.06093347e-01  -1.42857209e-01   6.06078625e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.33339953e+00  -1.42857209e-01\n",
      "   1.33341372e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.3977684276178479\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "steps=0\n",
    "while True:\n",
    "    steps+=1\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
    "# Paper: https://arxiv.org/abs/1802.09477\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action, hdlayer1=256, hdlayer2=128):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, hdlayer1)\n",
    "        self.l2 = nn.Linear(hdlayer1, hdlayer2)\n",
    "        self.l3 = nn.Linear(hdlayer2, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hdlayer1=256, hdlayer2=64):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, hdlayer1)\n",
    "        self.l2 = nn.Linear(hdlayer1, hdlayer2)\n",
    "        self.l3 = nn.Linear(hdlayer2, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, hdlayer1)\n",
    "        self.l5 = nn.Linear(hdlayer1, hdlayer2)\n",
    "        self.l6 = nn.Linear(hdlayer2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def _get_priority(self, error):\n",
    "        return (np.abs(error) + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        e = sample\n",
    "        #e = self.experience(sample)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = np.random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "        #print(is_weight.shape)\n",
    "        is_weight = torch.from_numpy(is_weight).float().to(device)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in batch if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in batch if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in batch if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in batch if e is not None])).float().to(device)\n",
    "        not_dones = torch.from_numpy(np.vstack([(1 - e.done) for e in batch if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        batch = (states, actions, rewards, next_states, not_dones)\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "        \n",
    "    def size(self):\n",
    "        return self.tree.n_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TD3(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        max_action,\n",
    "        discount=0.99,\n",
    "        tau=0.005,\n",
    "        policy_noise=0.2,\n",
    "        noise_clip=0.5,\n",
    "        policy_freq=2\n",
    "    ):\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        \n",
    "        # for delay update\n",
    "        self.policy_freq = policy_freq\n",
    "        self.total_it = 0\n",
    "\n",
    "        self.criticloss_hist = []\n",
    "        self.actorloss_hist = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def select_actions(self, states):\n",
    "        actions = []\n",
    "        for state in states:\n",
    "            state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "            actions.append(self.actor(state).cpu().data.numpy().flatten())\n",
    "        return actions\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=100):\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample replay buffer \n",
    "        exp, idxs, is_weight = replay_buffer.sample(batch_size)\n",
    "        state, action, reward, next_state, not_done = exp\n",
    "        #print ( state.shape, action.shape, next_state.shape, reward.shape, not_done.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (\n",
    "                torch.randn_like(action) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip)\n",
    "\n",
    "            #print (next_state.shape, noise.shape)\n",
    "            next_action = (\n",
    "                self.actor_target(next_state) + noise\n",
    "            ).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = (F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)).mean()\n",
    "        \n",
    "        # Compute critic loss\n",
    "        for Q1, Q2, TQ, idx, w in zip(current_Q1, current_Q2, target_Q, idxs, is_weight):\n",
    "            loss = (F.mse_loss(Q1, TQ).detach() + F.mse_loss(Q2, TQ).detach()) * w\n",
    "            replay_buffer.update(idx, loss)\n",
    "        \n",
    "        self.criticloss_hist.append(critic_loss)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # Compute actor losse\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            self.actorloss_hist.append(actor_loss)\n",
    "\n",
    "            # Optimize the actor \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # soft copy the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "\n",
    "        \n",
    "    def error(self, state, action, reward, next_state, done):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        next_state = torch.from_numpy(next_state).float().to(device)\n",
    "        action = torch.from_numpy(action).float().to(device)\n",
    "        critic_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            next_action = self.actor_target(next_state).clamp(-self.max_action, self.max_action)\n",
    "            #print (next_action.shape, next_action.shape)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            \n",
    "            #target_Q = reward + (1-np.array(done)) * self.discount * target_Q\n",
    "            target_Q = np.array(reward) + (1 - np.array(done)) * self.discount * target_Q.cpu().squeeze().numpy()\n",
    "            \n",
    "        target_Q = torch.from_numpy(target_Q).float().to(device)\n",
    "        #print (\"target_Q\", target_Q.shape)\n",
    "        \n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "        #print (\"current_Q1, Q2 \", current_Q1.shape, current_Q2.shape)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        for Q1, Q2, TQ, in zip(current_Q1, current_Q2, target_Q):\n",
    "            loss = F.mse_loss(Q1, TQ) + F.mse_loss(Q2, TQ)\n",
    "            critic_loss.append(loss.detach())\n",
    "        \n",
    "        critic_loss = np.array(critic_loss)\n",
    "        \n",
    "        #critic_loss = (F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)).mean().detach().numpy()\n",
    "\n",
    "        return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAverage Score: 0.49\n",
      "Episode 100\tAverage Score: 0.81\n",
      "Episode 150\tAverage Score: 0.93\n",
      "Episode 200\tAverage Score: 3.43\n",
      "Episode 250\tAverage Score: 6.44\n",
      "Episode 300\tAverage Score: 14.06\n",
      "Episode 350\tAverage Score: 16.98\n",
      "Episode 400\tAverage Score: 17.91\n",
      "Episode 450\tAverage Score: 21.47\n",
      "Episode 500\tAverage Score: 26.44\n",
      "Episode 550\tAverage Score: 24.70\n",
      "Episode 600\tAverage Score: 27.39\n",
      "Episode 650\tAverage Score: 30.57\n",
      "Episode 700\tAverage Score: 33.96\n",
      "Episode 750\tAverage Score: 35.69\n",
      "Episode 800\tAverage Score: 36.69\n",
      "Episode 850\tAverage Score: 38.78\n",
      "Episode 900\tAverage Score: 36.84\n",
      "Episode 950\tAverage Score: 36.65\n",
      "Episode 1000\tAverage Score: 38.09\n",
      "Episode 1050\tAverage Score: 37.73\n",
      "Episode 1100\tAverage Score: 38.96\n",
      "Episode 1150\tAverage Score: 39.00\n",
      "Episode 1200\tAverage Score: 38.60\n",
      "Episode 1250\tAverage Score: 41.84\n",
      "Episode 1300\tAverage Score: 42.39\n",
      "Episode 1350\tAverage Score: 42.15\n",
      "Episode 1400\tAverage Score: 41.09\n",
      "Episode 1450\tAverage Score: 42.03\n",
      "Episode 1500\tAverage Score: 45.68\n",
      "Episode 1550\tAverage Score: 43.41\n",
      "Episode 1600\tAverage Score: 48.98\n",
      "Episode 1650\tAverage Score: 47.72\n",
      "Episode 1675\tScore: 33.61"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c413d0d11882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Train agent after collecting sufficient data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mstart_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-aeefa851f208>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Sample replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#print ( state.shape, action.shape, next_state.shape, reward.shape, not_done.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-096ca834dd0e>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mnot_dones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "max_steps = 1001\n",
    "start_timesteps = 50\n",
    "batch_size = 256\n",
    "expl_noise = 0.05\n",
    "epochs = 2000\n",
    "max_action = 1\n",
    "print_every = 50\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "policy = TD3(state_dim=state_size, action_dim=action_size, max_action=1 )\n",
    "\n",
    "replay_buffer = ReplayBuffer(int(1e6))\n",
    "\n",
    "# Evaluate untrained policy\n",
    "#evaluations = [eval_policy(policy, args.env, args.seed)]\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "\n",
    "episode_reward = 0\n",
    "scores_deque = deque(maxlen=print_every)\n",
    "scores = []\n",
    "\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "for i_episode in range(1, epochs+1):\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    # Reset environment\n",
    "    episode_reward = 0\n",
    "            \n",
    "    for t in range(max_steps):\n",
    "\n",
    "        # Select action randomly or according to policy\n",
    "        if t < start_timesteps and i_episode < 100:\n",
    "            actions = np.random.normal(0, max_action, size=(len(states), action_size)).clip(-max_action, max_action)\n",
    "        else:\n",
    "            actions = (\n",
    "                policy.select_actions(np.array(states))\n",
    "                + np.random.normal(0, max_action * expl_noise, size=action_size)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        \n",
    "        errs = policy.error(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        # Store data in replay buffer\n",
    "        for state, action, reward, next_state, done, err in zip(states, actions, rewards, next_states, dones, errs):\n",
    "            #err = policy.error(state, action, reward, next_state, done)\n",
    "            exp = Experience(state, action, reward, next_state, done)\n",
    "            replay_buffer.add(err, exp)\n",
    "        \n",
    "        states = next_states\n",
    "        episode_reward += np.average(rewards)\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if replay_buffer.size() >= start_timesteps:\n",
    "            policy.train(replay_buffer, batch_size)\n",
    "\n",
    "        if np.any(dones): \n",
    "            break  \n",
    "            \n",
    "    scores_deque.append(episode_reward)\n",
    "    scores.append(episode_reward)\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.3f}\\tScore: {:.3f}\\tSteps: {:.4f}'.format(i_episode, np.mean(scores_deque), t), end=\"\")\n",
    "    if i_episode % print_every == 0:    \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.3f}\\tMax Score: {:.2f}'.format(i_episode, np.mean(scores_deque), np.max(scores_deque)))  \n",
    "        policy.save(\"actor_local\")\n",
    "    \n",
    "    if np.mean(scores_deque) > 60:\n",
    "        print('\\rFinished at Episode {}\\tReach Average Score: {:.2f}!'.format(i_episode, np.mean(scores_deque)))\n",
    "        break\n",
    "        \n",
    "policy.save(\"actor_local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save(\"Crawler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd78c6f0be0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_scores(scores, rolling_window=100):\n",
    "    \"\"\"Plot scores and optional rolling mean using specified window.\"\"\"\n",
    "    plt.plot(scores); plt.title(\"Scores\");\n",
    "    rolling_mean = pd.Series(scores).rolling(rolling_window).mean()\n",
    "    plt.plot(rolling_mean);\n",
    "    return rolling_mean\n",
    "\n",
    "rolling_mean = plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
